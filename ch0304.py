

# 3.4 softmax回归

# 3.4.1 分类问题
# 我们从一个图像分类问题开始, 假设每次输入是一个2 × 2的灰度图像, 可以用一个标量表示每个像素值，每个图像对应四个特征x1, x2, x3, x4
# 独热编码（one‐hot encoding）。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。在我们的例子中, 标签y将是一个三维向量，其中(1, 0, 0)对应于“猫”、 (0, 1, 0)对应于“鸡”、 (0, 0, 1)对应于“狗”：
#                                       y ∈ {(1, 0, 0), (0, 1, 0), (0, 0, 1)}

# 3.4.2 网络架构
# 为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出
# 为了解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affine function）。每个输出对应于它自己的仿射函数。
# 在我们的例子中，由于我们有4个特征和3个可能的输出类别，我们将需要12个标量来表示权重（带下标的w）， 3个标量来表示偏置（带下标的b）。
# 每个输入计算三个未规范化的预测（logit）： o1、 o2和o3
#                                       o1 = x1w11 + x2w12 + x3w13 + x4w14 + b1
#                                       o2 = x1w21 + x2w22 + x3w23 + x4w24 + b2                          
#                                       o3 = x1w31 + x2w32 + x3w33 + x4w34 + b3

# 与线性回归一样， softmax回归也是一个单层神经网络。
# 由于计算每个输出o1、 o2和o3取决于所有输入x1、 x2、 x3和x4，所以softmax回归的输出层也是全连接层

# 3.4.3 全连接层的参数开销
# 正如我们将在后续章节中看到的，在深度学习中，全连接层无处不在。然而，顾名思义，全连接层是“完全”连接的，可能有很多可学习的参数。
# 具体来说，对于任何具有d个输入和q个输出的全连接层，参数开销为O(dq)，这个数字在实践中可能高得令人望而却步。
# 幸运的是，将d个输入转换为q个输出的成本可以减少到O(dq/n)，其中超参数n可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性

# 3.4.4 softmax运算
# softmax函数: 首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。
# 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此， softmax回归是一个线性模型（linear model）。

# 3.4.5 小批量样本的矢量化
# 通常，在机器学习中，我们需要处理多个样本。如果我们有一个小批量的样本X，它的维度是n×d，这里n是样本的数量，d是每个样本的特征数量。
# 我们想要将这些样本通过一个参数化的模型进行转换，这个模型通常可以表示为权重矩阵W和偏置向量b。W的维度是d×q，b的维度是1×q，其中q是模型输出的维度。
# 模型的输出O计算为矩阵X和W的乘积加上偏置b（公式O=XW+b），之后通常会通过一个非线性函数处理，例如softmax函数，以得到分类问题的概率分布。
# softmax函数是一个常用的归一化指数函数，它能够将一个实数向量转换为一个概率分布。最后的输出Y是通过softmax函数处理O之后得到的，表示模型对每个样本的预测分类概率。

# 这里的矢量化操作允许我们不需要逐个处理每个样本，而是通过矩阵乘法一次性处理完整个小批量，这样可以更有效地利用现代计算硬件（尤其是GPU）的并行处理能力，从而加快模型的训练和推断过程。

# 3.4.6 损失函数
# 接下来，我们需要一个损失函数来度量预测的效果。我们将使用最大似然估计，这与在线性回归中的方法相同。
